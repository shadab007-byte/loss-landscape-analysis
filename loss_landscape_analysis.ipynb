{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchvision matplotlib seaborn scikit-learn\n",
        "\n",
        "print(\"✓ All packages installed!\")\n",
        "print(\"\\nVerifying GPU availability...\")\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DLB0UdCrGiA",
        "outputId": "c61b6a42-6cad-4f46-ad4d-ac368ec15c44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All packages installed!\n",
            "\n",
            "Verifying GPU availability...\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from scipy.stats import pearsonr\n",
        "import copy\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk15x35DrHED",
        "outputId": "6acb8af7-d1e7-4a95-9479-99a276f45e0a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaCNN(nn.Module):\n",
        "    \"\"\"Simple CNN for comparison\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VanillaCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with skip connection\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SimpleResNet(nn.Module):\n",
        "    \"\"\"Simple ResNet for CIFAR-10\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "print(\"✓ Model architectures defined!\")\n",
        "print(f\"  - Vanilla CNN: {sum(p.numel() for p in VanillaCNN().parameters())/1e6:.2f}M parameters\")\n",
        "print(f\"  - ResNet: {sum(p.numel() for p in SimpleResNet().parameters())/1e6:.2f}M parameters\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV7yeeRCrTY3",
        "outputId": "8e0f96d0-436b-4540-995a-44a70149adf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model architectures defined!\n",
            "  - Vanilla CNN: 2.47M parameters\n",
            "  - ResNet: 2.78M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from scipy.stats import pearsonr\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# ============================================================================\n",
        "# EFFICIENT LANDSCAPE PROBING METHODS\n",
        "# ============================================================================\n",
        "\n",
        "class LandscapeAnalyzer:\n",
        "    \"\"\"Core class for analyzing loss landscape geometry\"\"\"\n",
        "\n",
        "    def __init__(self, model, criterion, device='cuda'):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.device = device\n",
        "\n",
        "    def compute_loss_and_gradient(self, dataloader, subset_size=None):\n",
        "        \"\"\"Compute loss and gradient on dataset\"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0.0\n",
        "        gradients = {name: torch.zeros_like(param)\n",
        "                    for name, param in self.model.named_parameters()}\n",
        "\n",
        "        count = 0\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            if subset_size and batch_idx >= subset_size:\n",
        "                break\n",
        "\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, targets)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Compute gradients\n",
        "            self.model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if param.grad is not None:\n",
        "                    gradients[name] += param.grad.data\n",
        "\n",
        "            count += inputs.size(0)\n",
        "\n",
        "        avg_loss = total_loss / count\n",
        "        for name in gradients:\n",
        "            gradients[name] /= len(dataloader)\n",
        "\n",
        "        return avg_loss, gradients\n",
        "\n",
        "    def lanczos_hessian_spectrum(self, dataloader, num_eigenvalues=20,\n",
        "                                 num_iterations=100, subset_size=10):\n",
        "        \"\"\"\n",
        "        Compute top eigenvalues of Hessian using Lanczos algorithm\n",
        "        Memory-efficient: doesn't materialize full Hessian\n",
        "        \"\"\"\n",
        "        print(f\"Computing Hessian spectrum using Lanczos (k={num_eigenvalues})...\")\n",
        "\n",
        "        # Get parameter vector\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "        param_shapes = [p.shape for p in params]\n",
        "\n",
        "        def flatten_params(params_list):\n",
        "            return torch.cat([p.view(-1) for p in params_list])\n",
        "\n",
        "        def unflatten_to_params(flat_vec):\n",
        "            result = []\n",
        "            offset = 0\n",
        "            for shape in param_shapes:\n",
        "                numel = np.prod(shape)\n",
        "                result.append(flat_vec[offset:offset+numel].view(shape))\n",
        "                offset += numel\n",
        "            return result\n",
        "\n",
        "        def hessian_vector_product(vector):\n",
        "            \"\"\"Compute Hv using finite differences of gradients\"\"\"\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            # Compute gradient at current point\n",
        "            _, grad1 = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "            grad1_vec = flatten_params([grad1[name] for name, _ in self.model.named_parameters()])\n",
        "\n",
        "            # Small perturbation\n",
        "            epsilon = 1e-3\n",
        "            vector_as_params = unflatten_to_params(vector)\n",
        "\n",
        "            # Temporarily perturb parameters\n",
        "            with torch.no_grad():\n",
        "                for param, delta in zip(params, vector_as_params):\n",
        "                    param.add_(delta, alpha=epsilon)\n",
        "\n",
        "            # Compute gradient at perturbed point\n",
        "            _, grad2 = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "            grad2_vec = flatten_params([grad2[name] for name, _ in self.model.named_parameters()])\n",
        "\n",
        "            # Restore parameters\n",
        "            with torch.no_grad():\n",
        "                for param, delta in zip(params, vector_as_params):\n",
        "                    param.add_(delta, alpha=-epsilon)\n",
        "\n",
        "            # Hv ≈ (g(θ+εv) - g(θ)) / ε\n",
        "            hv = (grad2_vec - grad1_vec) / epsilon\n",
        "            return hv\n",
        "\n",
        "        # Lanczos iteration\n",
        "        param_vec = flatten_params(params)\n",
        "        n = param_vec.numel()\n",
        "\n",
        "        # Random starting vector\n",
        "        v = torch.randn_like(param_vec)\n",
        "        v = v / torch.norm(v)\n",
        "\n",
        "        # Tridiagonal matrix\n",
        "        alpha = []\n",
        "        beta = [0]\n",
        "        V = [v]\n",
        "\n",
        "        for i in range(min(num_eigenvalues, num_iterations)):\n",
        "            # Compute Hv\n",
        "            w = hessian_vector_product(V[-1])\n",
        "\n",
        "            # Orthogonalize\n",
        "            w = w - beta[-1] * (V[-2] if len(V) > 1 else 0)\n",
        "            alpha.append(torch.dot(w, V[-1]).item())\n",
        "            w = w - alpha[-1] * V[-1]\n",
        "\n",
        "            beta_new = torch.norm(w).item()\n",
        "            beta.append(beta_new)\n",
        "\n",
        "            if beta_new < 1e-10:\n",
        "                break\n",
        "\n",
        "            v_new = w / beta_new\n",
        "            V.append(v_new)\n",
        "\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"  Lanczos iteration {i+1}/{num_eigenvalues}\")\n",
        "\n",
        "        # Build tridiagonal matrix and compute eigenvalues\n",
        "        T = np.diag(alpha) + np.diag(beta[1:-1], k=1) + np.diag(beta[1:-1], k=-1)\n",
        "        eigenvalues = np.linalg.eigvalsh(T)\n",
        "        eigenvalues = sorted(eigenvalues, reverse=True)\n",
        "\n",
        "        return eigenvalues\n",
        "\n",
        "    def compute_sharpness(self, dataloader, rho=0.05, subset_size=10):\n",
        "        \"\"\"\n",
        "        Compute sharpness: max loss in ρ-ball around current parameters\n",
        "        Uses SAM-style ascent to find adversarial direction\n",
        "        \"\"\"\n",
        "        print(f\"Computing sharpness with ρ={rho}...\")\n",
        "\n",
        "        # Current loss\n",
        "        loss_current, _ = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "\n",
        "        # Compute adversarial perturbation (gradient ascent direction)\n",
        "        _, gradients = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "\n",
        "        # Normalize gradient to get direction\n",
        "        grad_norm = 0.0\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if name in gradients:\n",
        "                grad_norm += torch.sum(gradients[name] ** 2).item()\n",
        "        grad_norm = np.sqrt(grad_norm)\n",
        "\n",
        "        # Store original parameters\n",
        "        original_params = {name: param.clone()\n",
        "                          for name, param in self.model.named_parameters()}\n",
        "\n",
        "        # Perturb in adversarial direction\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if name in gradients and grad_norm > 0:\n",
        "                    perturbation = (rho / grad_norm) * gradients[name]\n",
        "                    param.add_(perturbation)\n",
        "\n",
        "        # Compute loss at perturbed point\n",
        "        loss_perturbed, _ = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "\n",
        "        # Restore original parameters\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.copy_(original_params[name])\n",
        "\n",
        "        sharpness = loss_perturbed - loss_current\n",
        "        return sharpness\n",
        "\n",
        "    def mode_connectivity(self, model2, dataloader, num_points=20):\n",
        "        \"\"\"\n",
        "        Analyze mode connectivity between self.model and model2\n",
        "        Returns: losses along linear interpolation path\n",
        "        \"\"\"\n",
        "        print(f\"Analyzing mode connectivity ({num_points} points)...\")\n",
        "\n",
        "        alphas = np.linspace(0, 1, num_points)\n",
        "        losses = []\n",
        "        accuracies = []\n",
        "\n",
        "        # Store original parameters\n",
        "        original_params = {name: param.clone()\n",
        "                          for name, param in self.model.named_parameters()}\n",
        "        model2_params = {name: param.clone()\n",
        "                        for name, param in model2.named_parameters()}\n",
        "\n",
        "        for alpha in tqdm(alphas, desc=\"Interpolation\"):\n",
        "            # Interpolate parameters: θ(α) = (1-α)θ₁ + αθ₂\n",
        "            with torch.no_grad():\n",
        "                for name, param in self.model.named_parameters():\n",
        "                    if name in model2_params:\n",
        "                        param.copy_((1 - alpha) * original_params[name] +\n",
        "                                   alpha * model2_params[name])\n",
        "\n",
        "            # Evaluate\n",
        "            self.model.eval()\n",
        "            total_loss = 0.0\n",
        "            correct = 0\n",
        "            total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in dataloader:\n",
        "                    inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self.criterion(outputs, targets)\n",
        "                    total_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                    _, predicted = outputs.max(1)\n",
        "                    total += targets.size(0)\n",
        "                    correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            losses.append(total_loss / total)\n",
        "            accuracies.append(100. * correct / total)\n",
        "\n",
        "        # Restore original parameters\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                param.copy_(original_params[name])\n",
        "\n",
        "        return alphas, losses, accuracies\n",
        "\n",
        "    def visualize_loss_surface_2d(self, dataloader, direction1=None, direction2=None,\n",
        "                                  range_=1.0, resolution=20, subset_size=5):\n",
        "        \"\"\"\n",
        "        Visualize 2D slice of loss surface around current parameters\n",
        "        If directions not provided, uses random directions\n",
        "        \"\"\"\n",
        "        print(f\"Computing 2D loss surface ({resolution}x{resolution} grid)...\")\n",
        "\n",
        "        # Get random directions if not provided\n",
        "        if direction1 is None or direction2 is None:\n",
        "            params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "            direction1 = [torch.randn_like(p) for p in params]\n",
        "            direction2 = [torch.randn_like(p) for p in params]\n",
        "\n",
        "            # Normalize directions\n",
        "            norm1 = np.sqrt(sum([torch.sum(d**2).item() for d in direction1]))\n",
        "            norm2 = np.sqrt(sum([torch.sum(d**2).item() for d in direction2]))\n",
        "            direction1 = [d / norm1 for d in direction1]\n",
        "            direction2 = [d / norm2 for d in direction2]\n",
        "\n",
        "            # Gram-Schmidt orthogonalization\n",
        "            dot_product = sum([torch.sum(d1 * d2).item()\n",
        "                             for d1, d2 in zip(direction1, direction2)])\n",
        "            direction2 = [d2 - dot_product * d1\n",
        "                         for d1, d2 in zip(direction1, direction2)]\n",
        "            norm2 = np.sqrt(sum([torch.sum(d**2).item() for d in direction2]))\n",
        "            direction2 = [d / norm2 for d in direction2]\n",
        "\n",
        "        # Store original parameters\n",
        "        original_params = [p.clone() for p in self.model.parameters() if p.requires_grad]\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "        # Create grid\n",
        "        alphas = np.linspace(-range_, range_, resolution)\n",
        "        betas = np.linspace(-range_, range_, resolution)\n",
        "\n",
        "        losses = np.zeros((resolution, resolution))\n",
        "\n",
        "        for i, alpha in enumerate(tqdm(alphas, desc=\"Computing surface\")):\n",
        "            for j, beta in enumerate(betas):\n",
        "                # Set parameters: θ = θ₀ + α*d₁ + β*d₂\n",
        "                with torch.no_grad():\n",
        "                    for k, param in enumerate(params):\n",
        "                        param.copy_(original_params[k] +\n",
        "                                   alpha * direction1[k] +\n",
        "                                   beta * direction2[k])\n",
        "\n",
        "                # Compute loss\n",
        "                loss, _ = self.compute_loss_and_gradient(dataloader, subset_size)\n",
        "                losses[i, j] = loss\n",
        "\n",
        "        # Restore original parameters\n",
        "        with torch.no_grad():\n",
        "            for k, param in enumerate(params):\n",
        "                param.copy_(original_params[k])\n",
        "\n",
        "        return alphas, betas, losses\n",
        "\n",
        "\n",
        "\n",
        "class VanillaCNN(nn.Module):\n",
        "    \"\"\"Simple CNN for comparison\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VanillaCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Residual block with skip connection\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, 1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SimpleResNet(nn.Module):\n",
        "    \"\"\"Simple ResNet for CIFAR-10\"\"\"\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.layer1 = self._make_layer(64, 64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(64, 128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(128, 256, 2, stride=2)\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, trainloader, testloader, device, epochs=20, lr=0.01):\n",
        "    \"\"\"Train a model and return training history\"\"\"\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'test_loss': [], 'test_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
        "        for inputs, targets in pbar:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "            pbar.set_postfix({'loss': train_loss/total, 'acc': 100.*correct/total})\n",
        "\n",
        "        history['train_loss'].append(train_loss / total)\n",
        "        history['train_acc'].append(100. * correct / total)\n",
        "\n",
        "        # Testing\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in testloader:\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "                test_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += targets.size(0)\n",
        "                correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        history['test_loss'].append(test_loss / total)\n",
        "        history['test_acc'].append(100. * correct / total)\n",
        "\n",
        "        print(f'Epoch {epoch+1}: Test Loss: {test_loss/total:.4f}, Test Acc: {100.*correct/total:.2f}%')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"Evaluate model accuracy and loss\"\"\"\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "\n",
        "def run_complete_analysis():\n",
        "    \"\"\"\n",
        "    Main function to run complete landscape analysis\n",
        "\n",
        "    This will:\n",
        "    1. Train models with different architectures\n",
        "    2. Compute landscape metrics (sharpness, Hessian spectrum, etc.)\n",
        "    3. Analyze mode connectivity\n",
        "    4. Visualize loss surfaces\n",
        "    5. Correlate geometry with generalization\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"LOSS LANDSCAPE GEOMETRY & OPTIMIZATION DYNAMICS\")\n",
        "    print(\"Complete Analysis Framework\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Setup\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nUsing device: {device}\")\n",
        "\n",
        "    # Data loading\n",
        "    print(\"\\n[1/7] Loading CIFAR-10 dataset...\")\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                           download=True, transform=transform_train)\n",
        "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                          download=True, transform=transform_test)\n",
        "    testloader = DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # For landscape analysis (smaller subset for efficiency)\n",
        "    subset_indices = np.random.choice(len(testset), 1000, replace=False)\n",
        "    subset = torch.utils.data.Subset(testset, subset_indices)\n",
        "    subsetloader = DataLoader(subset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Train models\n",
        "    print(\"\\n[2/7] Training models...\")\n",
        "\n",
        "    print(\"\\n  Training Vanilla CNN...\")\n",
        "    vanilla_model = VanillaCNN(num_classes=10)\n",
        "    vanilla_history = train_model(vanilla_model, trainloader, testloader, device, epochs=20)\n",
        "\n",
        "    print(\"\\n  Training ResNet...\")\n",
        "    resnet_model = SimpleResNet(num_classes=10)\n",
        "    resnet_history = train_model(resnet_model, trainloader, testloader, device, epochs=20)\n",
        "\n",
        "    # Train a second instance for mode connectivity\n",
        "    print(\"\\n  Training second ResNet instance for mode connectivity...\")\n",
        "    resnet_model2 = SimpleResNet(num_classes=10)\n",
        "    resnet_history2 = train_model(resnet_model2, trainloader, testloader, device, epochs=20)\n",
        "\n",
        "    # Landscape analysis\n",
        "    results = {}\n",
        "\n",
        "    print(\"\\n[3/7] Computing landscape metrics for Vanilla CNN...\")\n",
        "    vanilla_analyzer = LandscapeAnalyzer(vanilla_model, nn.CrossEntropyLoss(), device)\n",
        "\n",
        "    vanilla_sharpness = vanilla_analyzer.compute_sharpness(subsetloader, rho=0.05)\n",
        "    vanilla_eigenvalues = vanilla_analyzer.lanczos_hessian_spectrum(subsetloader, num_eigenvalues=20)\n",
        "    vanilla_loss, vanilla_acc = evaluate_model(vanilla_model, testloader, device)\n",
        "\n",
        "    results['vanilla'] = {\n",
        "        'sharpness': vanilla_sharpness,\n",
        "        'eigenvalues': vanilla_eigenvalues,\n",
        "        'test_loss': vanilla_loss,\n",
        "        'test_acc': vanilla_acc,\n",
        "        'history': vanilla_history\n",
        "    }\n",
        "\n",
        "    print(\"\\n[4/7] Computing landscape metrics for ResNet...\")\n",
        "    resnet_analyzer = LandscapeAnalyzer(resnet_model, nn.CrossEntropyLoss(), device)\n",
        "\n",
        "    resnet_sharpness = resnet_analyzer.compute_sharpness(subsetloader, rho=0.05)\n",
        "    resnet_eigenvalues = resnet_analyzer.lanczos_hessian_spectrum(subsetloader, num_eigenvalues=20)\n",
        "    resnet_loss, resnet_acc = evaluate_model(resnet_model, testloader, device)\n",
        "\n",
        "    results['resnet'] = {\n",
        "        'sharpness': resnet_sharpness,\n",
        "        'eigenvalues': resnet_eigenvalues,\n",
        "        'test_loss': resnet_loss,\n",
        "        'test_acc': resnet_acc,\n",
        "        'history': resnet_history\n",
        "    }\n",
        "\n",
        "    print(\"\\n[5/7] Analyzing mode connectivity...\")\n",
        "    alphas, losses, accuracies = resnet_analyzer.mode_connectivity(\n",
        "        resnet_model2, testloader, num_points=15\n",
        "    )\n",
        "\n",
        "    results['mode_connectivity'] = {\n",
        "        'alphas': alphas,\n",
        "        'losses': losses,\n",
        "        'accuracies': accuracies\n",
        "    }\n",
        "\n",
        "    print(\"\\n[6/7] Computing 2D loss surface visualizations...\")\n",
        "    vanilla_x, vanilla_y, vanilla_surface = vanilla_analyzer.visualize_loss_surface_2d(\n",
        "        subsetloader, range_=0.5, resolution=15\n",
        "    )\n",
        "\n",
        "    resnet_x, resnet_y, resnet_surface = resnet_analyzer.visualize_loss_surface_2d(\n",
        "        subsetloader, range_=0.5, resolution=15\n",
        "    )\n",
        "\n",
        "    results['vanilla']['surface'] = (vanilla_x, vanilla_y, vanilla_surface)\n",
        "    results['resnet']['surface'] = (resnet_x, resnet_y, resnet_surface)\n",
        "\n",
        "    print(\"\\n[7/7] Saving results...\")\n",
        "    with open('landscape_analysis_results.pkl', 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    # Print summary\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ANALYSIS SUMMARY\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nVanilla CNN:\")\n",
        "    print(f\"  Test Accuracy: {vanilla_acc:.2f}%\")\n",
        "    print(f\"  Test Loss: {vanilla_loss:.4f}\")\n",
        "    print(f\"  Sharpness (ρ=0.05): {vanilla_sharpness:.6f}\")\n",
        "    print(f\"  Max Hessian eigenvalue: {vanilla_eigenvalues[0]:.4f}\")\n",
        "    print(f\"  Min Hessian eigenvalue: {vanilla_eigenvalues[-1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nResNet:\")\n",
        "    print(f\"  Test Accuracy: {resnet_acc:.2f}%\")\n",
        "    print(f\"  Test Loss: {resnet_loss:.4f}\")\n",
        "    print(f\"  Sharpness (ρ=0.05): {resnet_sharpness:.6f}\")\n",
        "    print(f\"  Max Hessian eigenvalue: {resnet_eigenvalues[0]:.4f}\")\n",
        "    print(f\"  Min Hessian eigenvalue: {resnet_eigenvalues[-1]:.4f}\")\n",
        "\n",
        "    print(f\"\\nMode Connectivity:\")\n",
        "    print(f\"  Max barrier height: {max(losses) - min(losses):.4f}\")\n",
        "    print(f\"  Min accuracy along path: {min(accuracies):.2f}%\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    results = run_complete_analysis()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Analysis complete! Results saved to 'landscape_analysis_results.pkl'\")\n",
        "    print(\"Run the visualization script next to generate plots.\")\n",
        "    print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnSoKdr8rYlU",
        "outputId": "0a7f59dc-7174-484a-aa17-8d8373b12985"
      },
      "execution_count": 5,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "LOSS LANDSCAPE GEOMETRY & OPTIMIZATION DYNAMICS\n",
            "Complete Analysis Framework\n",
            "================================================================================\n",
            "\n",
            "Using device: cuda\n",
            "\n",
            "[1/7] Loading CIFAR-10 dataset...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170M/170M [09:37<00:00, 295kB/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[2/7] Training models...\n",
            "\n",
            "  Training Vanilla CNN...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [00:19<00:00, 19.98it/s, loss=1.82, acc=33]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Test Loss: 1.4353, Test Acc: 47.67%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 391/391 [00:19<00:00, 20.47it/s, loss=1.43, acc=47.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Test Loss: 1.2193, Test Acc: 55.29%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 391/391 [00:19<00:00, 19.62it/s, loss=1.22, acc=56.2]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Test Loss: 1.0408, Test Acc: 63.17%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 391/391 [00:20<00:00, 19.09it/s, loss=1.06, acc=62.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Test Loss: 0.8945, Test Acc: 68.58%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 391/391 [00:18<00:00, 20.96it/s, loss=0.956, acc=66.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Test Loss: 0.8116, Test Acc: 71.88%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 391/391 [00:19<00:00, 20.07it/s, loss=0.879, acc=69.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Test Loss: 0.7674, Test Acc: 73.28%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 391/391 [00:18<00:00, 21.36it/s, loss=0.822, acc=71.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Test Loss: 0.7283, Test Acc: 74.80%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 391/391 [00:18<00:00, 20.96it/s, loss=0.76, acc=73.2]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Test Loss: 0.7124, Test Acc: 75.18%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 391/391 [00:19<00:00, 19.74it/s, loss=0.724, acc=74.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Test Loss: 0.6742, Test Acc: 76.72%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 391/391 [00:18<00:00, 21.08it/s, loss=0.691, acc=76]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Test Loss: 0.6165, Test Acc: 78.89%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 391/391 [00:19<00:00, 20.52it/s, loss=0.65, acc=77.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Test Loss: 0.6076, Test Acc: 79.33%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 391/391 [00:19<00:00, 19.81it/s, loss=0.624, acc=78.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Test Loss: 0.5916, Test Acc: 79.54%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 391/391 [00:18<00:00, 20.79it/s, loss=0.597, acc=79.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Test Loss: 0.5698, Test Acc: 80.56%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/20: 100%|██████████| 391/391 [00:18<00:00, 20.65it/s, loss=0.574, acc=80]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Test Loss: 0.5445, Test Acc: 80.98%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/20: 100%|██████████| 391/391 [00:19<00:00, 20.28it/s, loss=0.555, acc=80.8]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Test Loss: 0.5478, Test Acc: 81.04%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/20: 100%|██████████| 391/391 [00:18<00:00, 21.26it/s, loss=0.539, acc=81.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Test Loss: 0.5385, Test Acc: 81.61%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/20: 100%|██████████| 391/391 [00:18<00:00, 20.97it/s, loss=0.521, acc=81.9]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Test Loss: 0.5247, Test Acc: 81.93%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/20: 100%|██████████| 391/391 [00:18<00:00, 20.63it/s, loss=0.51, acc=82.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Test Loss: 0.5151, Test Acc: 82.26%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/20: 100%|██████████| 391/391 [00:17<00:00, 21.74it/s, loss=0.506, acc=82.5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Test Loss: 0.5127, Test Acc: 82.57%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/20: 100%|██████████| 391/391 [00:19<00:00, 20.45it/s, loss=0.502, acc=82.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20: Test Loss: 0.5088, Test Acc: 82.43%\n",
            "\n",
            "  Training ResNet...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [00:35<00:00, 10.87it/s, loss=1.41, acc=48.2]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Test Loss: 1.2249, Test Acc: 58.05%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 391/391 [00:34<00:00, 11.23it/s, loss=0.946, acc=66.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Test Loss: 1.1949, Test Acc: 62.43%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.74, acc=74]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Test Loss: 0.6899, Test Acc: 75.91%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s, loss=0.621, acc=78.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Test Loss: 0.7209, Test Acc: 75.08%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 391/391 [00:35<00:00, 11.13it/s, loss=0.53, acc=81.6]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Test Loss: 0.7877, Test Acc: 74.82%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 391/391 [00:35<00:00, 11.09it/s, loss=0.476, acc=83.5]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Test Loss: 0.6213, Test Acc: 80.06%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.424, acc=85.4]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Test Loss: 0.6372, Test Acc: 79.04%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/20: 100%|██████████| 391/391 [00:34<00:00, 11.22it/s, loss=0.383, acc=86.7]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Test Loss: 0.5131, Test Acc: 83.12%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/20: 100%|██████████| 391/391 [00:35<00:00, 11.09it/s, loss=0.35, acc=87.9]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Test Loss: 0.5236, Test Acc: 82.87%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/20: 100%|██████████| 391/391 [00:35<00:00, 11.01it/s, loss=0.316, acc=89.1]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Test Loss: 0.6296, Test Acc: 79.95%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/20: 100%|██████████| 391/391 [00:34<00:00, 11.18it/s, loss=0.283, acc=90.2]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Test Loss: 0.4428, Test Acc: 85.90%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/20: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s, loss=0.257, acc=91]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Test Loss: 0.4278, Test Acc: 86.48%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/20: 100%|██████████| 391/391 [00:34<00:00, 11.18it/s, loss=0.229, acc=92.3]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Test Loss: 0.3751, Test Acc: 87.91%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.203, acc=93.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Test Loss: 0.3409, Test Acc: 88.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 391/391 [00:35<00:00, 11.11it/s, loss=0.179, acc=94]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Test Loss: 0.3458, Test Acc: 88.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 391/391 [00:35<00:00, 11.05it/s, loss=0.161, acc=94.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Test Loss: 0.3104, Test Acc: 89.81%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.145, acc=95.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Test Loss: 0.3101, Test Acc: 89.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.131, acc=95.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Test Loss: 0.3033, Test Acc: 90.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 391/391 [00:34<00:00, 11.22it/s, loss=0.125, acc=96.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Test Loss: 0.2945, Test Acc: 90.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 391/391 [00:35<00:00, 11.11it/s, loss=0.121, acc=96.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Test Loss: 0.2944, Test Acc: 90.42%\n",
            "\n",
            "  Training second ResNet instance for mode connectivity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 391/391 [00:34<00:00, 11.17it/s, loss=1.43, acc=47.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Test Loss: 1.3985, Test Acc: 52.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.938, acc=66.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Test Loss: 0.9752, Test Acc: 68.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 391/391 [00:35<00:00, 11.14it/s, loss=0.728, acc=74.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Test Loss: 0.8979, Test Acc: 71.51%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.61, acc=78.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Test Loss: 0.7500, Test Acc: 75.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s, loss=0.536, acc=81.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Test Loss: 0.5853, Test Acc: 80.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 391/391 [00:34<00:00, 11.19it/s, loss=0.476, acc=83.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Test Loss: 0.6183, Test Acc: 79.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s, loss=0.425, acc=85.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Test Loss: 0.5576, Test Acc: 81.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s, loss=0.386, acc=86.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Test Loss: 0.5527, Test Acc: 81.84%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 391/391 [00:35<00:00, 11.15it/s, loss=0.345, acc=88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Test Loss: 0.4654, Test Acc: 84.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 391/391 [00:35<00:00, 11.12it/s, loss=0.311, acc=89.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: Test Loss: 0.5063, Test Acc: 83.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 391/391 [00:35<00:00, 11.17it/s, loss=0.281, acc=90.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: Test Loss: 0.4321, Test Acc: 85.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.251, acc=91.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: Test Loss: 0.4179, Test Acc: 86.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 391/391 [00:34<00:00, 11.20it/s, loss=0.226, acc=92.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: Test Loss: 0.3717, Test Acc: 87.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.201, acc=93.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: Test Loss: 0.3336, Test Acc: 89.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 391/391 [00:35<00:00, 11.15it/s, loss=0.177, acc=94.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: Test Loss: 0.3276, Test Acc: 89.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 391/391 [00:34<00:00, 11.18it/s, loss=0.16, acc=94.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: Test Loss: 0.3178, Test Acc: 89.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 391/391 [00:35<00:00, 11.16it/s, loss=0.143, acc=95.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: Test Loss: 0.3110, Test Acc: 89.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 391/391 [00:34<00:00, 11.18it/s, loss=0.132, acc=95.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: Test Loss: 0.3002, Test Acc: 90.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 391/391 [00:35<00:00, 11.13it/s, loss=0.123, acc=96.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: Test Loss: 0.2937, Test Acc: 90.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 391/391 [00:34<00:00, 11.18it/s, loss=0.117, acc=96.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: Test Loss: 0.2924, Test Acc: 90.42%\n",
            "\n",
            "[3/7] Computing landscape metrics for Vanilla CNN...\n",
            "Computing sharpness with ρ=0.05...\n",
            "Computing Hessian spectrum using Lanczos (k=20)...\n",
            "  Lanczos iteration 10/20\n",
            "  Lanczos iteration 20/20\n",
            "\n",
            "[4/7] Computing landscape metrics for ResNet...\n",
            "Computing sharpness with ρ=0.05...\n",
            "Computing Hessian spectrum using Lanczos (k=20)...\n",
            "  Lanczos iteration 10/20\n",
            "  Lanczos iteration 20/20\n",
            "\n",
            "[5/7] Analyzing mode connectivity...\n",
            "Analyzing mode connectivity (15 points)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Interpolation: 100%|██████████| 15/15 [00:41<00:00,  2.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[6/7] Computing 2D loss surface visualizations...\n",
            "Computing 2D loss surface (15x15 grid)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing surface: 100%|██████████| 15/15 [00:57<00:00,  3.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing 2D loss surface (15x15 grid)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing surface: 100%|██████████| 15/15 [01:41<00:00,  6.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[7/7] Saving results...\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "Vanilla CNN:\n",
            "  Test Accuracy: 82.43%\n",
            "  Test Loss: 0.5088\n",
            "  Sharpness (ρ=0.05): 0.071657\n",
            "  Max Hessian eigenvalue: 107.8411\n",
            "  Min Hessian eigenvalue: -95.4789\n",
            "\n",
            "ResNet:\n",
            "  Test Accuracy: 90.42%\n",
            "  Test Loss: 0.2944\n",
            "  Sharpness (ρ=0.05): 1.287901\n",
            "  Max Hessian eigenvalue: 1613.7731\n",
            "  Min Hessian eigenvalue: -283.3368\n",
            "\n",
            "Mode Connectivity:\n",
            "  Max barrier height: 16.6086\n",
            "  Min accuracy along path: 10.00%\n",
            "\n",
            "================================================================================\n",
            "Analysis complete! Results saved to 'landscape_analysis_results.pkl'\n",
            "Run the visualization script next to generate plots.\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from scipy.stats import pearsonr\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "def load_results():\n",
        "    \"\"\"Load analysis results\"\"\"\n",
        "    with open('landscape_analysis_results.pkl', 'rb') as f:\n",
        "        results = pickle.load(f)\n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_training_curves(results):\n",
        "    \"\"\"Plot training and test curves\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "    models = ['vanilla', 'resnet']\n",
        "    titles = ['Vanilla CNN', 'ResNet']\n",
        "\n",
        "    for idx, (model, title) in enumerate(zip(models, titles)):\n",
        "        history = results[model]['history']\n",
        "        epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "        # Loss curves\n",
        "        axes[0, idx].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "        axes[0, idx].plot(epochs, history['test_loss'], 'r-', label='Test Loss', linewidth=2)\n",
        "        axes[0, idx].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[0, idx].set_ylabel('Loss', fontsize=12)\n",
        "        axes[0, idx].set_title(f'{title} - Loss Curves', fontsize=14, fontweight='bold')\n",
        "        axes[0, idx].legend(fontsize=11)\n",
        "        axes[0, idx].grid(True, alpha=0.3)\n",
        "\n",
        "        # Accuracy curves\n",
        "        axes[1, idx].plot(epochs, history['train_acc'], 'b-', label='Train Accuracy', linewidth=2)\n",
        "        axes[1, idx].plot(epochs, history['test_acc'], 'r-', label='Test Accuracy', linewidth=2)\n",
        "        axes[1, idx].set_xlabel('Epoch', fontsize=12)\n",
        "        axes[1, idx].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "        axes[1, idx].set_title(f'{title} - Accuracy Curves', fontsize=14, fontweight='bold')\n",
        "        axes[1, idx].legend(fontsize=11)\n",
        "        axes[1, idx].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: training_curves.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_hessian_spectrum(results):\n",
        "    \"\"\"Plot Hessian eigenvalue spectrum\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    models = ['vanilla', 'resnet']\n",
        "    titles = ['Vanilla CNN', 'ResNet']\n",
        "    colors = ['#e74c3c', '#3498db']\n",
        "\n",
        "    for idx, (model, title, color) in enumerate(zip(models, titles, colors)):\n",
        "        eigenvalues = results[model]['eigenvalues']\n",
        "        indices = range(1, len(eigenvalues) + 1)\n",
        "\n",
        "        axes[idx].bar(indices, eigenvalues, color=color, alpha=0.7, edgecolor='black')\n",
        "        axes[idx].axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
        "        axes[idx].set_xlabel('Eigenvalue Index', fontsize=12)\n",
        "        axes[idx].set_ylabel('Eigenvalue Magnitude', fontsize=12)\n",
        "        axes[idx].set_title(f'{title} - Hessian Spectrum', fontsize=14, fontweight='bold')\n",
        "        axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "        # Add statistics\n",
        "        max_eig = eigenvalues[0]\n",
        "        min_eig = eigenvalues[-1]\n",
        "        n_negative = sum(1 for e in eigenvalues if e < 0)\n",
        "\n",
        "        textstr = f'Max λ: {max_eig:.2f}\\nMin λ: {min_eig:.2f}\\n# Negative: {n_negative}'\n",
        "        axes[idx].text(0.98, 0.97, textstr, transform=axes[idx].transAxes,\n",
        "                      fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
        "                      bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('hessian_spectrum.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: hessian_spectrum.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_comparison_metrics(results):\n",
        "\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "    models = ['Vanilla\\nCNN', 'ResNet']\n",
        "\n",
        "    # Sharpness\n",
        "    sharpness_values = [results['vanilla']['sharpness'], results['resnet']['sharpness']]\n",
        "    bars1 = axes[0].bar(models, sharpness_values, color=['#e74c3c', '#3498db'], alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    axes[0].set_ylabel('Sharpness (ρ=0.05)', fontsize=12)\n",
        "    axes[0].set_title('Loss Sharpness Comparison', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar in bars1:\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    # Max eigenvalue\n",
        "    max_eig_values = [results['vanilla']['eigenvalues'][0], results['resnet']['eigenvalues'][0]]\n",
        "    bars2 = axes[1].bar(models, max_eig_values, color=['#e74c3c', '#3498db'], alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    axes[1].set_ylabel('Max Eigenvalue (λₘₐₓ)', fontsize=12)\n",
        "    axes[1].set_title('Maximum Hessian Eigenvalue', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for bar in bars2:\n",
        "        height = bar.get_height()\n",
        "        axes[1].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    # Test accuracy\n",
        "    acc_values = [results['vanilla']['test_acc'], results['resnet']['test_acc']]\n",
        "    bars3 = axes[2].bar(models, acc_values, color=['#e74c3c', '#3498db'], alpha=0.7, edgecolor='black', linewidth=2)\n",
        "    axes[2].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    axes[2].set_title('Generalization Performance', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_ylim([min(acc_values)-5, 100])\n",
        "    axes[2].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    for bar in bars3:\n",
        "        height = bar.get_height()\n",
        "        axes[2].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.2f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: metrics_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_mode_connectivity(results):\n",
        "\n",
        "    mc = results['mode_connectivity']\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # Loss along path\n",
        "    axes[0].plot(mc['alphas'], mc['losses'], 'b-o', linewidth=2, markersize=6)\n",
        "    axes[0].set_xlabel('Interpolation Parameter α', fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title('Loss Along Linear Interpolation Path', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Model 1')\n",
        "    axes[0].axvline(x=1, color='green', linestyle='--', alpha=0.5, label='Model 2')\n",
        "    axes[0].legend(fontsize=11)\n",
        "\n",
        "    # Mark barrier\n",
        "    max_loss_idx = np.argmax(mc['losses'])\n",
        "    axes[0].plot(mc['alphas'][max_loss_idx], mc['losses'][max_loss_idx],\n",
        "                'r*', markersize=15, label='Barrier Peak')\n",
        "\n",
        "    barrier_height = max(mc['losses']) - min(mc['losses'])\n",
        "    textstr = f'Barrier Height: {barrier_height:.4f}'\n",
        "    axes[0].text(0.5, 0.95, textstr, transform=axes[0].transAxes,\n",
        "                fontsize=11, verticalalignment='top', horizontalalignment='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))\n",
        "\n",
        "    # Accuracy along path\n",
        "    axes[1].plot(mc['alphas'], mc['accuracies'], 'g-o', linewidth=2, markersize=6)\n",
        "    axes[1].set_xlabel('Interpolation Parameter α', fontsize=12)\n",
        "    axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    axes[1].set_title('Accuracy Along Linear Interpolation Path', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].axvline(x=0, color='red', linestyle='--', alpha=0.5, label='Model 1')\n",
        "    axes[1].axvline(x=1, color='green', linestyle='--', alpha=0.5, label='Model 2')\n",
        "    axes[1].legend(fontsize=11)\n",
        "\n",
        "    min_acc_idx = np.argmin(mc['accuracies'])\n",
        "    axes[1].plot(mc['alphas'][min_acc_idx], mc['accuracies'][min_acc_idx],\n",
        "                'r*', markersize=15, label='Accuracy Dip')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('mode_connectivity.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: mode_connectivity.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_loss_surface_3d(results):\n",
        "    \"\"\"Plot 3D loss surface visualization\"\"\"\n",
        "    fig = plt.figure(figsize=(16, 6))\n",
        "\n",
        "    models = ['vanilla', 'resnet']\n",
        "    titles = ['Vanilla CNN - Loss Surface', 'ResNet - Loss Surface']\n",
        "\n",
        "    for idx, (model, title) in enumerate(zip(models, titles)):\n",
        "        x, y, z = results[model]['surface']\n",
        "\n",
        "        ax = fig.add_subplot(1, 2, idx+1, projection='3d')\n",
        "\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "\n",
        "        # Plot surface\n",
        "        surf = ax.plot_surface(X, Y, z.T, cmap=cm.viridis,\n",
        "                              linewidth=0, antialiased=True, alpha=0.8)\n",
        "\n",
        "        # Mark minimum\n",
        "        min_idx = np.unravel_index(z.argmin(), z.shape)\n",
        "        ax.scatter([x[min_idx[0]]], [y[min_idx[1]]], [z[min_idx[0], min_idx[1]]],\n",
        "                  color='red', s=100, marker='*', label='Minimum')\n",
        "\n",
        "        ax.set_xlabel('Direction 1', fontsize=11)\n",
        "        ax.set_ylabel('Direction 2', fontsize=11)\n",
        "        ax.set_zlabel('Loss', fontsize=11)\n",
        "        ax.set_title(title, fontsize=13, fontweight='bold', pad=20)\n",
        "\n",
        "        # Colorbar\n",
        "        fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
        "\n",
        "        ax.view_init(elev=25, azim=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_surface_3d.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: loss_surface_3d.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_loss_surface_contour(results):\n",
        "    \"\"\"Plot contour plots of loss surface\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    models = ['vanilla', 'resnet']\n",
        "    titles = ['Vanilla CNN', 'ResNet']\n",
        "\n",
        "    for idx, (model, title) in enumerate(zip(models, titles)):\n",
        "        x, y, z = results[model]['surface']\n",
        "\n",
        "        X, Y = np.meshgrid(x, y)\n",
        "\n",
        "        # Contour plot\n",
        "        contour = axes[idx].contourf(X, Y, z.T, levels=20, cmap='viridis')\n",
        "        axes[idx].contour(X, Y, z.T, levels=20, colors='black', alpha=0.3, linewidths=0.5)\n",
        "\n",
        "        # Mark minimum\n",
        "        min_idx = np.unravel_index(z.argmin(), z.shape)\n",
        "        axes[idx].plot(x[min_idx[0]], y[min_idx[1]], 'r*', markersize=15, label='Minimum')\n",
        "\n",
        "        axes[idx].set_xlabel('Direction 1', fontsize=12)\n",
        "        axes[idx].set_ylabel('Direction 2', fontsize=12)\n",
        "        axes[idx].set_title(f'{title} - Loss Contours', fontsize=14, fontweight='bold')\n",
        "        axes[idx].legend(fontsize=11)\n",
        "\n",
        "        # Colorbar\n",
        "        plt.colorbar(contour, ax=axes[idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('loss_surface_contour.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: loss_surface_contour.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_correlation_analysis(results):\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "\n",
        "    models_data = {\n",
        "        'Vanilla CNN': results['vanilla'],\n",
        "        'ResNet': results['resnet']\n",
        "    }\n",
        "\n",
        "    sharpness = [data['sharpness'] for data in models_data.values()]\n",
        "    test_acc = [data['test_acc'] for data in models_data.values()]\n",
        "    max_eig = [data['eigenvalues'][0] for data in models_data.values()]\n",
        "\n",
        "    # Sharpness vs Accuracy\n",
        "    axes[0].scatter(sharpness, test_acc, s=200, alpha=0.7, c=['#e74c3c', '#3498db'],\n",
        "                   edgecolors='black', linewidth=2)\n",
        "\n",
        "    for i, (name, sharp, acc) in enumerate(zip(models_data.keys(), sharpness, test_acc)):\n",
        "        axes[0].annotate(name, (sharp, acc), xytext=(10, -10),\n",
        "                        textcoords='offset points', fontsize=10,\n",
        "                        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    # Trend line\n",
        "    z = np.polyfit(sharpness, test_acc, 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_trend = np.linspace(min(sharpness), max(sharpness), 100)\n",
        "    axes[0].plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
        "\n",
        "    # Correlation coefficient\n",
        "    if len(sharpness) > 1:\n",
        "        corr, _ = pearsonr(sharpness, test_acc)\n",
        "        axes[0].text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "                    transform=axes[0].transAxes, fontsize=11,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    axes[0].set_xlabel('Sharpness', fontsize=12)\n",
        "    axes[0].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    axes[0].set_title('Sharpness vs Generalization', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend(fontsize=11)\n",
        "\n",
        "    # Max Eigenvalue vs Accuracy\n",
        "    axes[1].scatter(max_eig, test_acc, s=200, alpha=0.7, c=['#e74c3c', '#3498db'],\n",
        "                   edgecolors='black', linewidth=2)\n",
        "\n",
        "    for i, (name, eig, acc) in enumerate(zip(models_data.keys(), max_eig, test_acc)):\n",
        "        axes[1].annotate(name, (eig, acc), xytext=(10, -10),\n",
        "                        textcoords='offset points', fontsize=10,\n",
        "                        bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.5))\n",
        "\n",
        "    # Trend line\n",
        "    z = np.polyfit(max_eig, test_acc, 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_trend = np.linspace(min(max_eig), max(max_eig), 100)\n",
        "    axes[1].plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2, label='Trend')\n",
        "\n",
        "    # Correlation coefficient\n",
        "    if len(max_eig) > 1:\n",
        "        corr, _ = pearsonr(max_eig, test_acc)\n",
        "        axes[1].text(0.05, 0.95, f'Correlation: {corr:.3f}',\n",
        "                    transform=axes[1].transAxes, fontsize=11,\n",
        "                    verticalalignment='top',\n",
        "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    axes[1].set_xlabel('Max Hessian Eigenvalue', fontsize=12)\n",
        "    axes[1].set_ylabel('Test Accuracy (%)', fontsize=12)\n",
        "    axes[1].set_title('Curvature vs Generalization', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend(fontsize=11)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: correlation_analysis.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def generate_summary_table(results):\n",
        "    \"\"\"Generate and save summary table as image\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 4))\n",
        "    ax.axis('tight')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Prepare data\n",
        "    metrics = [\n",
        "        'Test Accuracy (%)',\n",
        "        'Test Loss',\n",
        "        'Sharpness (ρ=0.05)',\n",
        "        'Max Eigenvalue (λₚₐₓ)',\n",
        "        'Min Eigenvalue (λₘᵢₙ)',\n",
        "        '# Negative Eigenvalues'\n",
        "    ]\n",
        "\n",
        "    vanilla_data = [\n",
        "        f\"{results['vanilla']['test_acc']:.2f}\",\n",
        "        f\"{results['vanilla']['test_loss']:.4f}\",\n",
        "        f\"{results['vanilla']['sharpness']:.6f}\",\n",
        "        f\"{results['vanilla']['eigenvalues'][0]:.4f}\",\n",
        "        f\"{results['vanilla']['eigenvalues'][-1]:.4f}\",\n",
        "        f\"{sum(1 for e in results['vanilla']['eigenvalues'] if e < 0)}\"\n",
        "    ]\n",
        "\n",
        "    resnet_data = [\n",
        "        f\"{results['resnet']['test_acc']:.2f}\",\n",
        "        f\"{results['resnet']['test_loss']:.4f}\",\n",
        "        f\"{results['resnet']['sharpness']:.6f}\",\n",
        "        f\"{results['resnet']['eigenvalues'][0]:.4f}\",\n",
        "        f\"{results['resnet']['eigenvalues'][-1]:.4f}\",\n",
        "        f\"{sum(1 for e in results['resnet']['eigenvalues'] if e < 0)}\"\n",
        "    ]\n",
        "\n",
        "    table_data = []\n",
        "    for i, metric in enumerate(metrics):\n",
        "        table_data.append([metric, vanilla_data[i], resnet_data[i]])\n",
        "\n",
        "    table = ax.table(cellText=table_data,\n",
        "                    colLabels=['Metric', 'Vanilla CNN', 'ResNet'],\n",
        "                    cellLoc='center',\n",
        "                    loc='center',\n",
        "                    colWidths=[0.4, 0.3, 0.3])\n",
        "\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 2.5)\n",
        "\n",
        "    # Style header\n",
        "    for i in range(3):\n",
        "        table[(0, i)].set_facecolor('#4CAF50')\n",
        "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
        "\n",
        "    # Alternate row colors\n",
        "    for i in range(1, len(metrics) + 1):\n",
        "        for j in range(3):\n",
        "            if i % 2 == 0:\n",
        "                table[(i, j)].set_facecolor('#f0f0f0')\n",
        "\n",
        "    plt.title('Summary of Landscape Metrics', fontsize=16, fontweight='bold', pad=20)\n",
        "    plt.savefig('summary_table.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"Saved: summary_table.png\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Generate all visualizations\"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\"GENERATING VISUALIZATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(\"\\nLoading results...\")\n",
        "    results = load_results()\n",
        "\n",
        "    print(\"\\nGenerating plots...\")\n",
        "    plot_training_curves(results)\n",
        "    plot_hessian_spectrum(results)\n",
        "    plot_comparison_metrics(results)\n",
        "    plot_mode_connectivity(results)\n",
        "    plot_loss_surface_3d(results)\n",
        "    plot_loss_surface_contour(results)\n",
        "    plot_correlation_analysis(results)\n",
        "    generate_summary_table(results)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ALL VISUALIZATIONS COMPLETE!\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nGenerated files:\")\n",
        "    print(\"  1. training_curves.png\")\n",
        "    print(\"  2. hessian_spectrum.png\")\n",
        "    print(\"  3. metrics_comparison.png\")\n",
        "    print(\"  4. mode_connectivity.png\")\n",
        "    print(\"  5. loss_surface_3d.png\")\n",
        "    print(\"  6. loss_surface_contour.png\")\n",
        "    print(\"  7. correlation_analysis.png\")\n",
        "    print(\"  8. summary_table.png\")\n",
        "    print(\"\\nUse these figures in your final report!\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f10EXpusmOh",
        "outputId": "d8cb1e42-1b66-4a42-c602-0f17a8ab2ef7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "GENERATING VISUALIZATIONS\n",
            "================================================================================\n",
            "\n",
            "Loading results...\n",
            "\n",
            "Generating plots...\n",
            "Saved: training_curves.png\n",
            "Saved: hessian_spectrum.png\n",
            "Saved: metrics_comparison.png\n",
            "Saved: mode_connectivity.png\n",
            "Saved: loss_surface_3d.png\n",
            "Saved: loss_surface_contour.png\n",
            "Saved: correlation_analysis.png\n",
            "Saved: summary_table.png\n",
            "\n",
            "================================================================================\n",
            "ALL VISUALIZATIONS COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "Generated files:\n",
            "  1. training_curves.png\n",
            "  2. hessian_spectrum.png\n",
            "  3. metrics_comparison.png\n",
            "  4. mode_connectivity.png\n",
            "  5. loss_surface_3d.png\n",
            "  6. loss_surface_contour.png\n",
            "  7. correlation_analysis.png\n",
            "  8. summary_table.png\n",
            "\n",
            "Use these figures in your final report!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_vBeYlg2BS6"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}